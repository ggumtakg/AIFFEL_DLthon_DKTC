{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jbZCM9Zkwpxd",
    "outputId": "ea4f7564-4069-472a-9b62-53c41787697f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ggumtakg/py-hanspell\n",
      "  Cloning https://github.com/ggumtakg/py-hanspell to /tmp/pip-req-build-tjumjo56\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ggumtakg/py-hanspell /tmp/pip-req-build-tjumjo56\n",
      "  Resolved https://github.com/ggumtakg/py-hanspell to commit 3bc19f0cfc61d158afc46a250aad4230eaad6435\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from py-hanspell==1.1) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->py-hanspell==1.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->py-hanspell==1.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->py-hanspell==1.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->py-hanspell==1.1) (2024.8.30)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: konlpy in /opt/conda/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from konlpy) (1.5.1)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.11/site-packages (from konlpy) (5.3.0)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.11/site-packages (from konlpy) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/ggumtakg/py-hanspell\n",
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPlfly1ly34l"
   },
   "source": [
    "# EDA & 전처리\n",
    "- 데이터 샘플 및 기초정보 확인\n",
    "- 특수문자 삭제 및 영문 소문자화\n",
    "- /n을 띄워쓰기로 변환\n",
    "- 욕설 필터링\n",
    "- KoNLPy의 형태소 분석기(mecab)를 통한 토크나이징 후\n",
    "  - 텍스트 길이 분포 확인\n",
    "  - 단어 빈도 분석\n",
    "  - 불용어 처리 -> 불용어는 형태소분석기를 통한 토크나이징 후 해당 불용어만 존재하는 토큰을 삭제\n",
    "- 결측값 제거\n",
    "- 중복데이터 제거\n",
    "- 단어 임베딩\n",
    "- 패딩을 통한 길이 통일\n",
    "- 원핫 인코딩을 통한 클래스 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vgbRal3swr8I"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from konlpy.tag import Mecab  # 형태소 분석기\n",
    "from collections import Counter # 빈도 분석\n",
    "from wordcloud import WordCloud # 빈도 시각화\n",
    "from gensim.models import Word2Vec  # 단어 임베딩\n",
    "from sklearn.model_selection import train_test_split  # train, validation분리\n",
    "from hanspell import spell_checker  # 맞춤법 검사기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDoQe5u0y8si"
   },
   "source": [
    "### 데이터 로드 및 기초정보 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UL-fIBC-cASC",
    "outputId": "3e4cdfbc-0997-4788-fc1f-c6e776e3a74a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idx  class                                       conversation\n",
      "0    0  일반 대화  너희 오토바이 타봤어? 이번에 친구가 오토바이 사서 자랑하는데 너무 부럽더라 나는 ...\n",
      "1    1  일반 대화  나 연수좀 도와줘 운전 연수 말하는거야? 하...또 이 베스트드라이버가 나설 차레인...\n",
      "2    2  일반 대화  나 이번 주에 할머니 집 가. 그래? 오랜만에 할머니도 보고 좋겠네. 그러게, 할머...\n",
      "3    3  일반 대화  학교 등교 할 때 택시타고 등교 하는데 차가 너무 많이 막혀 맞아, 요즘 차가 너무...\n",
      "4    4  일반 대화  오늘 등굣길에 버스를 탔는데 사람이 너무 많았어. 학생들 등교 시간이라 사람이 많은...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4950 entries, 0 to 4949\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   idx           4950 non-null   int64 \n",
      " 1   class         4950 non-null   object\n",
      " 2   conversation  4950 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 116.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "train_data = pd.read_csv('/workspace/train.csv')\n",
    "\n",
    "# 데이터 샘플 확인\n",
    "print(train_data.head())\n",
    "\n",
    "# 기본 정보 확인 - null유무 및 데이터타입, 갯수 확인\n",
    "print(train_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06zQnJfIz1t7"
   },
   "source": [
    "### 텍스트 정규화\n",
    "- 문장 양극단 공백 제거\n",
    "- 과도하게 반복되는 문자 제거(ex. 키키키키키키 -> 키키)\n",
    "- 불용 특수문자 삭제\n",
    "- 영문 소문자화\n",
    "- 여러개의 공백 하나로 치환(ex. 아니   뭐래 -> 아니 뭐래)\n",
    "- 줄바꿈 문자 띄워쓰기로 치환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "cU663SNozV4B",
    "outputId": "250b688c-58a2-4e57-e32a-c6f2d136057d"
   },
   "outputs": [],
   "source": [
    "# 맞춤법 검사기 초기화\n",
    "spell_checker.setParam('8940e67396b6ae4844dfea8112e130c6e2d0e2f5', '1733453547941')\n",
    "\n",
    "# 특수문자 삭제 및 영문 소문자화 함수\n",
    "def preprocess_text(text):\n",
    "    # 입력받은 문장의 양쪽 공백을 제거\n",
    "    text = text.strip()\n",
    "\n",
    "    # 반복 문자 제거\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "\n",
    "    # 특수문자 삭제\n",
    "    text = re.sub(r'[^가-힣a-zA-Z0-9?\\s]', '', text)\n",
    "\n",
    "    # 영문 소문자화\n",
    "    text = text.lower()\n",
    "\n",
    "    # 여러 개의 공백을 하나의 공백으로 변환\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # 입력받은 문장의 양쪽 공백을 제거\n",
    "    text = text.strip()\n",
    "\n",
    "    # 줄바꿈문자를 띄워쓰기로 변환\n",
    "    text.replace('\\n', ' ')\n",
    "\n",
    "    # 한글 맞춤법 교정\n",
    "    corrected_sentences = []\n",
    "    for sentence in text:\n",
    "        corrected_sentence = spell_checker.check(sentence).checked\n",
    "        corrected_sentences.append(corrected_sentence)\n",
    "    return corrected_sentences\n",
    "\n",
    "# 텍스트 전처리\n",
    "train_data['conversation'] = train_data['conversation'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tZ3gXoE4Qki"
   },
   "outputs": [],
   "source": [
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoL_HPv0-y1k"
   },
   "source": [
    "### 여기까지 진행된 데이터 저장 및 이후로는 저장된 데이터 활용\n",
    "- 맞춤법 검사기로 인한 실행시간이 길어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zy8OQlZ_-yK9"
   },
   "outputs": [],
   "source": [
    "train_data.to_csv('train_preproc.csv', index=False, encoding='utf-8')\n",
    "\n",
    "train_data = pd.read_csv('train_preproc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNH0Aiyl1TNw"
   },
   "source": [
    "### 욕설 필터링\n",
    "- 욕설 단어만 제거\n",
    "- 욕설 목록은 https://namu.wiki/w/%EC%9A%95%EC%84%A4/%ED%95%9C%EA%B5%AD%EC%96%B4를 참고하여 일부 발췌 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nc-Om4B01Wfb"
   },
   "outputs": [],
   "source": [
    "'''사용 여부에 따라 주석 해제 요망\n",
    "# 욕설 리스트\n",
    "bad_words = ['씨발', '개새끼', '니미']\n",
    "\n",
    "# 욕설 제거 함수\n",
    "def remove_bad_words(text):\n",
    "    for bad_word in bad_words:\n",
    "        text = text.replace(bad_word, '')\n",
    "    return text\n",
    "\n",
    "# 데이터 로드\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# 욕설 제거\n",
    "train_data['text'] = train_data['text'].apply(remove_bad_words)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCyQP7fL2wh6"
   },
   "source": [
    "### 형태소 분석기\n",
    "- KoNLPy의 Mecab사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHsZk-932oMS"
   },
   "outputs": [],
   "source": [
    "# 형태소 분석기 초기화\n",
    "mecab = Mecab()\n",
    "\n",
    "# 형태소 분석 함수\n",
    "def tokenize_text(text):\n",
    "    return mecab.morphs(text)\n",
    "\n",
    "# 형태소 분석\n",
    "train_data['tokens'] = train_data['conversation'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WF_f39gc-YTb"
   },
   "source": [
    "### 불용어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uejzpabX-Ymr"
   },
   "outputs": [],
   "source": [
    "# 불용어 리스트\n",
    "stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
    "\n",
    "# 불용어 제거 함수\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stopwords]\n",
    "\n",
    "# 불용어 제거\n",
    "train_data['tokens'] = train_data['tokens'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P96F7akx3Nju"
   },
   "source": [
    "### 텍스트 길이 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GjesQ5t3NI5"
   },
   "outputs": [],
   "source": [
    "# 텍스트 길이 분포 확인\n",
    "train_data['text_length'] = train_data['tokens'].apply(len)\n",
    "\n",
    "sns.histplot(train_data['text_length'], bins=50, kde=True)\n",
    "plt.title('Text Length Distribution in Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvDmX68L-BEo"
   },
   "source": [
    "### 단어 빈도 분석\n",
    "- wordcloud를 통한 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yY1qqasg-AmY"
   },
   "outputs": [],
   "source": [
    "# 단어 빈도 분석\n",
    "train_words = [word for tokens in train_data['tokens'] for word in tokens]\n",
    "train_word_freq = Counter(train_words)\n",
    "\n",
    "# wordcloud 시각화\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', font_path='path/to/your/font.ttf').generate_from_frequencies(train_word_freq)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6CI-u7P-PF_"
   },
   "source": [
    "### 결측치 및 중복 데이터 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgdaVz2S-lm4"
   },
   "outputs": [],
   "source": [
    "# 결측값 제거\n",
    "train_data.dropna(inplace=True)\n",
    "\n",
    "# 중복 데이터 제거\n",
    "train_data['conversation'].drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBS20vRpAC9A"
   },
   "source": [
    "### 단어 임베딩\n",
    "- word2vec 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b6biqtsAGhg"
   },
   "outputs": [],
   "source": [
    "# Word2Vec 모델 학습\n",
    "sentences = train_data['tokens'].tolist()\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 단어 임베딩 벡터 생성\n",
    "def get_word_embedding(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return [0] * model.vector_size\n",
    "\n",
    "# 단어 임베딩 벡터 생성\n",
    "train_data['embedding'] = train_data['tokens'].apply(lambda x: get_word_embedding(x, word2vec_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B60fB2OlAKsv"
   },
   "source": [
    "### pad_sequences를 통한 패딩 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "webzRQGnALAy"
   },
   "outputs": [],
   "source": [
    "# 패딩 함수\n",
    "MAX_LENGTH = 100\n",
    "def pad_sequences(sequences, maxlen=MAX_LENGTH, padding='post', truncating='post'):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > maxlen:\n",
    "            seq = seq[:maxlen]\n",
    "        else:\n",
    "            seq = seq + [0] * (maxlen - len(seq))\n",
    "        padded_sequences.append(seq)\n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "# 패딩 적용\n",
    "train_embeddings = pad_sequences(train_data['embedding'].tolist(), maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1X9r9BB8AuTI"
   },
   "source": [
    "### 클래스 원 핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsjjOgpSAul1"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 원핫 인코딩\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "train_labels = encoder.fit_transform(train_data[['class']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbuqsVplBBtE"
   },
   "source": [
    "### train, validation 데이터셋 분리\n",
    "- 비율 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kk3jEgOUBCBU"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_embeddings, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터셋 준비\n",
    "train_dataset = (X_train, y_train)\n",
    "val_dataset = (X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GHJMDnFB4Ck"
   },
   "source": [
    "### 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7ixojvpB5-q"
   },
   "outputs": [],
   "source": [
    "train_dataset.to_csv('train_dataset.csv', index=False, encoding='utf-8')\n",
    "val_dataset.to_csv('val_dataset.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
